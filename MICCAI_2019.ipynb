{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MICCAI 2019",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "piRsc9rYYRzl"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uld1IQgwKIL",
        "colab_type": "text"
      },
      "source": [
        "# Medical Deep Learning 2D high resolution image segmentation project:\n",
        "## MICCAI 2019 Prostate Cancer segmentation challenge\n",
        "### Data can be downloaded from here: https://gleason2019.grand-challenge.org/\n",
        "\n",
        "#### In order to reproduce the results of this challenge place the extracted data in a folder and use it as root path "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-oHUemNtm59",
        "colab_type": "text"
      },
      "source": [
        "## 1. Mount Drive\n",
        "Mount the google drive to access the dataset stored on drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYkOcDYNtf-e",
        "colab_type": "code",
        "outputId": "c7129a28-5260-41c0-dc69-fdd9747a619d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!nvidia-smi\n",
        "!free -m\n",
        "\n",
        "# unzip challenge data in a folder and change path accordingly\n",
        "root_path = '/gdrive/My Drive/MICCAI_2019_pathology_challenge/' "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "Mon Sep  2 06:35:17 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:          26126         595       23484           0        2045       25171\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8UsZGYmS85V",
        "colab_type": "text"
      },
      "source": [
        "## 2. Install Requirements and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmalSM-iS7Tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip3 install imageio\n",
        "!pip3 install torchsummary\n",
        " \n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ARH9U4D2Wy",
        "colab_type": "text"
      },
      "source": [
        "## 3. Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEIRGXpdfssT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_img(img_path):\n",
        "  \"\"\"\n",
        "  Reads a .png image and returns it as a numpy array.\n",
        "  \"\"\"\n",
        "  return imageio.imread(img_path)\n",
        "\n",
        "def check_path_in_list(path, list):\n",
        "  \"\"\"\n",
        "  Checks a path if exist in the other list\n",
        "  \"\"\"\n",
        "  key = path.split('/')[-1]\n",
        "  for full_path in list:\n",
        "    path_id = full_path.split('/')[-1]\n",
        "    if path_id==key:\n",
        "      image_numpy = read_img(full_path)\n",
        "      return image_numpy\n",
        "    return None\n",
        "  \n",
        "def get_majority_vote(a):\n",
        "  \"\"\"\n",
        "  Returns the majority vote element of a list\n",
        "  \"\"\"\n",
        "  return max(map(lambda val: (a.count(val), val), set(a)))[1]\n",
        "  \n",
        "def vote(stacked_labels):\n",
        "    \"\"\"\n",
        "    Performs majority voting on the stacked labels\n",
        "    \"\"\"\n",
        "    voters, height, width = stacked_labels.shape\n",
        "    final_labels = stacked_labels.sum(axis=0)\n",
        "        \n",
        "    for i in range(height):\n",
        "      for j in range(width):\n",
        "        votes = stacked_labels[:,i,j]\n",
        "        value = get_majority_vote(votes.tolist())\n",
        "        final_labels[i,j] = value\n",
        "    print('original: ', np.unique(stacked_labels),'voted: ', np.unique(final_labels))    \n",
        "    return final_labels\n",
        "\n",
        "def preprocess_labels(maps_tuple):\n",
        "    \"\"\"\n",
        "    Majority labeling vote to produce ground truth labels\n",
        "    \"\"\"\n",
        "    label_list = []\n",
        "    \n",
        "    m1, m2, m3, m4, m5, m6 = maps_tuple\n",
        "    for j in range(len(m5)):\n",
        "      path = m5[j] # as a reference annotation\n",
        "      \n",
        "      key = path.split('/')[-1]\n",
        "      \n",
        "      image_list = []\n",
        "      # voter 5\n",
        "      image_list.append(read_img(path))\n",
        "              \n",
        "      #voter 1\n",
        "      image = check_path_in_list(path, m1)\n",
        "      if image is not None:\n",
        "        image_list.append(image)\n",
        "      \n",
        "      # voter 2\n",
        "      image = check_path_in_list(path, m2)\n",
        "      if image is not None:\n",
        "        image_list.append(image)\n",
        "      \n",
        "      # voter 3\n",
        "      image = check_path_in_list(path, m3)\n",
        "      if image is not None:\n",
        "        image_list.append(image)\n",
        "      \n",
        "      # voter 4\n",
        "      image = check_path_in_list(path, m4)\n",
        "      if image is not None:\n",
        "        image_list.append(image)\n",
        "\n",
        "      #voter 6         \n",
        "      image = check_path_in_list(path, m6)\n",
        "      if image is not None:\n",
        "        image_list.append(image)\n",
        "       \n",
        "      stacked_labels = np.stack(image_list,axis=0)\n",
        "      label = vote(stacked_labels)\n",
        "      imageio.imwrite('./labels/'+key, label.astype('uint8') )\n",
        "      \n",
        "def read_labels(root_path):\n",
        "    \"\"\"\n",
        "    Reads labels and returns them in a tuple of sorted lists\n",
        "    \"\"\"\n",
        "    label_list = []\n",
        "    map_1 = sorted(glob.glob(root_path+'Maps1_T/Maps1_T/*.png'))\n",
        "    map_2 = sorted(glob.glob(root_path+'Maps2_T/Maps2_T/*.png'))\n",
        "    map_3 = sorted(glob.glob(root_path+'Maps3_T/Maps3_T/*.png'))\n",
        "    map_4 = sorted(glob.glob(root_path+'Maps4_T/Maps4_T/*.png'))\n",
        "    map_5 = sorted(glob.glob(root_path+'Maps5_T/Maps5_T/*.png'))\n",
        "    map_6 = sorted(glob.glob(root_path+'Maps6_T/Maps6_T/*.png'))\n",
        "    return map_1, map_2, map_3, map_4, map_5, map_6\n",
        "  \n",
        "def shuffle_lists(a, b):\n",
        "  c = list(zip(a, b))\n",
        "  random.shuffle(c)\n",
        "  a, b = zip(*c)\n",
        "  return a, b\n",
        "\n",
        "def datestr():\n",
        "    now = time.gmtime()\n",
        "    return '{}{:02}{:02}_{:02}{:02}'.format(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min)\n",
        "\n",
        "def save_checkpoint(state, is_best, path, prefix, filename='checkpoint.pth.tar'):\n",
        "    prefix_save = os.path.join(path, prefix)\n",
        "    name = prefix_save + '_' + filename\n",
        "    torch.save(state, name)\n",
        "    if is_best:\n",
        "      shutil.copyfile(name, prefix_save + '_model_best.pth.tar')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piRsc9rYYRzl",
        "colab_type": "text"
      },
      "source": [
        "## 4. Read annotations and offline processing \n",
        "\n",
        "### Applies majority voting for the provided annotations to generate training labels\n",
        "\n",
        "#### Executed only once due to poor time complexity (rougly 2-3 minutes to generate 1 image label)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7adjWzdCuECK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir labels\n",
        "root_path = '/gdrive/My Drive/MICCAI_2019_pathology_challenge/'\n",
        "tuple_maps = read_labels(root_path)\n",
        "preprocess_labels(tuple_maps)\n",
        "print(\"DONE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAl6xITKHvDT",
        "colab_type": "text"
      },
      "source": [
        "### Download generated labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iELlzircHfA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "!zip -r ./labels_final.zip ./labels\n",
        "files.download(\"./labels_final.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDUaRfhh1ygZ",
        "colab_type": "text"
      },
      "source": [
        "#### After downloading the images you should unzip them and place them in a folder named 'Labels' in the dataset path (root path)\n",
        "#### Alternatively, use this link: https://drive.google.com/drive/folders/16nIihgeFe0ZS9KeKbzUwSZzRFX4fRljC?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojc7YgwC305t",
        "colab_type": "text"
      },
      "source": [
        "## 5. Define Model + Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSXLS-xXdpKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2D-Unet Model taken from https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_model.py\n",
        "class DoubleConv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(InConv, self).__init__()\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(Down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(Up, self).__init__()\n",
        "\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        \n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
        "                        diffY // 2, diffY - diffY//2))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "      \n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(Unet, self).__init__()\n",
        "        self.inc = InConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 512)\n",
        "        self.up1 = Up(1024, 256)\n",
        "        self.up2 = Up(512, 128)\n",
        "        self.up3 = Up(256, 64)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return torch.sigmoid(x)\n",
        "      \n",
        "      \n",
        "# Borrowed and changed from my repository https://github.com/black0017/MedicalZooPytorch/blob/master/src/medical_zoo.py\n",
        "class DiceLoss2D(nn.Module):\n",
        "    def __init__(self, classes, epsilon=1e-5, sigmoid_normalization=True):\n",
        "        super(DiceLoss2D, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.classes = classes\n",
        "        \n",
        "        if sigmoid_normalization:\n",
        "            self.normalization = nn.Sigmoid()\n",
        "        else:\n",
        "            self.normalization = nn.Softmax(dim=0)\n",
        "        \n",
        "    def flatten(self, tensor):\n",
        "        return tensor.view(self.classes, -1)\n",
        "\n",
        "    def expand_as_one_hot(self, target):\n",
        "        \"\"\"\n",
        "        Converts label image to CxHxW, where each label gets converted to\n",
        "        its corresponding one-hot vector\n",
        "        :param target is of shape  (1xHxW)\n",
        "        :return: 3D output tensor (CxHxW) where C is the classes\n",
        "        \"\"\"\n",
        "        shape = target.size()\n",
        "        shape = list(shape)\n",
        "        shape.insert(1, self.classes)\n",
        "        shape = tuple(shape)\n",
        "        # expand the input tensor to Nx1xHxW\n",
        "        src = target.unsqueeze(0)\n",
        "        return torch.zeros(shape).to(target.device).scatter_(1, src, 1).squeeze(0)\n",
        "\n",
        "    def compute_per_channel_dice(self, input, target):\n",
        "        epsilon = 1e-5\n",
        "        target = self.expand_as_one_hot(target.long())\n",
        "        assert input.size() == target.size(),\"input' and 'target' must have the same shape\"\n",
        "\n",
        "        input = self.flatten(input)\n",
        "        target = self.flatten(target).float()\n",
        "\n",
        "        # Compute per channel Dice Coefficient\n",
        "        intersect = (input * target).sum(-1)\n",
        "        denominator = (input + target).sum(-1)\n",
        "        return 2. * intersect / denominator.clamp(min=epsilon)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input = self.normalization(input)\n",
        "        per_channel_dice = self.compute_per_channel_dice(input, target)\n",
        "        DSC = per_channel_dice.clone().cpu().detach().numpy()\n",
        "        # Average the Dice score across all channels/classes\n",
        "        return torch.mean(1. - per_channel_dice), DSC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DlgGBn41ZEB",
        "colab_type": "text"
      },
      "source": [
        "## 6. Generate Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2-UThXP1L-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Gleason2019(Dataset):\n",
        "    \"\"\"\n",
        "    Code for reading Gleason 2019 MICCAI Challenge\n",
        "    \"\"\"\n",
        "    def __init__(self, mode, image_paths, label_paths, split = (0.8, 0.2), crop_dim=(512, 512), samples=100):\n",
        "        \"\"\"\n",
        "        :param mode: 'train','val'\n",
        "        :param image_paths: image dataset paths\n",
        "        :param label_paths: label dataset paths\n",
        "        :param crop_dim: 2 element tuple to decide crop values\n",
        "        :param samples: number of sub-grids to create(patches of the input img)\n",
        "        \"\"\"\n",
        "        \n",
        "        self.slices = 244\n",
        "        self.mode = mode\n",
        "        self.crop_dim = crop_dim\n",
        "        self.sample_list = []\n",
        "        self.samples = samples\n",
        "        train_idx = int(split[0]*self.slices)\n",
        "        val_idx = int(split[1]*self.slices)\n",
        "        \n",
        "        if self.mode == 'train':\n",
        "          self.list_imgs = image_paths[0:train_idx]\n",
        "          self.list_labels = label_paths[0:train_idx]\n",
        "          self.generate_samples()\n",
        "        elif self.mode == 'val':\n",
        "          self.list_imgs = image_paths[train_idx:(train_idx+val_idx)]\n",
        "          self.list_labels = label_paths[train_idx:(train_idx+val_idx)]\n",
        "          self.generate_samples()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tuple_in = self.sample_list[index]\n",
        "        img_tensor, segmentation_map = tuple_in\n",
        "        return img_tensor, segmentation_map\n",
        "\n",
        "    def generate_samples(self):\n",
        "        total = len(self.list_imgs)\n",
        "        print('Total ' + self.mode + ' data to generate samples:', total)\n",
        "        for j in range(total):\n",
        "          for i in range(self.samples):\n",
        "            input_path = self.list_imgs[j]\n",
        "            label_path = self.list_labels[j]\n",
        "            \n",
        "            img_numpy = imageio.imread(input_path)\n",
        "            label_numpy = imageio.imread(label_path)\n",
        "            \n",
        "            h_crop, w_crop = self.generate_patch(img_numpy)\n",
        "            img_numpy = img_numpy[h_crop:(h_crop + self.crop_dim[0]),\n",
        "                                w_crop:(w_crop + self.crop_dim[1]), :]\n",
        "            \n",
        "            label_numpy = label_numpy[h_crop:(h_crop + self.crop_dim[0]),\n",
        "                                w_crop:(w_crop + self.crop_dim[1])]\n",
        "            \n",
        "            img_tensor = torch.from_numpy(img_numpy).float()\n",
        "            label_tensor = torch.from_numpy(label_numpy)\n",
        "            \n",
        "            img_tensor = img_tensor.permute(2, 0, 1)\n",
        "            img_tensor = self.norm_img(img_tensor)\n",
        "            self.sample_list.append(tuple((img_tensor, label_tensor)))\n",
        "\n",
        "    def generate_patch(self, img):    \n",
        "      h, w, c = img.shape\n",
        "      if h<self.crop_dim[0] or w<self.crop_dim[1]:\n",
        "        print('dim error')\n",
        "        print(h,self.crop_dim[0],w,self.crop_dim[1])\n",
        "      h_crop = np.random.randint(h - self.crop_dim[0])\n",
        "      w_crop = np.random.randint(w - self.crop_dim[1])\n",
        "      return h_crop, w_crop\n",
        "    \n",
        "    def norm_img(self, img_tensor):\n",
        "      mask = img_tensor.ne(0.0)\n",
        "      desired = img_tensor[mask]\n",
        "      mean_val, std_val = desired.mean(), desired.std()\n",
        "      img_tensor = (img_tensor - mean_val)/std_val\n",
        "      return img_tensor "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biZdbk7-zImW",
        "colab_type": "text"
      },
      "source": [
        "## 7. Train & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ocYfLtnzNe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, model, trainLoader, optimizer, criterion, trainF):\n",
        "  model.train()\n",
        "  n_train = len(trainLoader.dataset)\n",
        "  train_loss = 0\n",
        "  dice_avg_coeff = 0\n",
        "\n",
        "  for batch_idx, input_tuple in enumerate(trainLoader):\n",
        "      optimizer.zero_grad()\n",
        "      input_tensor, target = input_tuple\n",
        "      input_tensor.requires_grad = True\n",
        "      input_tensor, target = input_tensor.cuda(), target.cuda()\n",
        "      output = model(input_tensor)\n",
        "      loss_dice, per_ch_score = criterion(output.squeeze(0), target)\n",
        "      loss_dice.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Keep Training statistics\n",
        "      dice_coeff = 100.*(1. - loss_dice.item())\n",
        "      train_loss += loss_dice.item()\n",
        "      dice_avg_coeff += dice_coeff\n",
        "      \n",
        "  # Mean statistics\n",
        "  train_loss = train_loss/n_train\n",
        "  dice_avg_coeff = dice_avg_coeff/n_train\n",
        "  print('\\n Train Epoch {} Summary:  \\t Dice Loss: {:.4f}\\t AVG Dice Coeff: {:.4f}'\n",
        "         .format(epoch, train_loss, dice_avg_coeff))\n",
        "  \n",
        "  trainF.write('{},{},{}\\n'.format(epoch, train_loss, dice_avg_coeff ))\n",
        "  trainF.flush()\n",
        "\n",
        "def evaluate(epoch, model, val_generator, optimizer, criterion, val_f):\n",
        "  model.eval()\n",
        "  n_val = len(val_generator.dataset)\n",
        "  val_loss = 0\n",
        "  dice_avg_coeff = 0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, input_tuple in enumerate(val_generator):\n",
        "        input_tensor, target = input_tuple\n",
        "        input_tensor, target = input_tensor.cuda(), target.cuda()\n",
        "        output = model(input_tensor)\n",
        "        \n",
        "        loss_dice, per_ch_score = criterion(output.squeeze(0), target)\n",
        "        # Keep Training statistics\n",
        "        dice_coeff = 100.*(1. - loss_dice.item())\n",
        "        val_loss += loss_dice.item()\n",
        "        dice_avg_coeff += dice_coeff\n",
        "  \n",
        "    # Mean statistics\n",
        "    val_loss = val_loss/n_val\n",
        "    dice_avg_coeff = dice_avg_coeff/n_val\n",
        "    print('\\n Validation Epoch {} Summary:  \\t Dice Loss: {:.4f}\\t AVG Dice Coeff: {:.4f}'.format(epoch, val_loss, dice_avg_coeff))\n",
        "  \n",
        "  val_f.write('{},{},{}\\n'.format(epoch, val_loss, dice_avg_coeff ))\n",
        "  val_f.flush() \n",
        "  return val_loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HfEJegm1r39",
        "colab_type": "text"
      },
      "source": [
        "## Main - Putting it all together!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buJRuqHb1rZO",
        "colab_type": "code",
        "outputId": "8a5dabd1-0b79-4d2a-cb80-903654a90527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classes = 7 # 0-->6\n",
        "epochs = 10\n",
        "subepochs = 50\n",
        "dim = 512 # rectangular crop size\n",
        "samples_per_image_train = 1\n",
        "samples_per_image_val = 3\n",
        "in_channels = 3\n",
        "lr = 0.001 \n",
        "\n",
        "# Data preparation\n",
        "train_imgs = sorted(glob.glob(root_path + 'Train Imgs/Train Imgs/*.jpg'))\n",
        "labels_final = sorted(glob.glob(root_path + 'Labels/*.png'))\n",
        "\n",
        "train_imgs, labels_final = shuffle_lists(train_imgs, labels_final)\n",
        "\n",
        "# Model\n",
        "model = Unet(in_channels,classes)\n",
        "model = model.cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
        "criterion = DiceLoss2D(classes)\n",
        "\n",
        "best_pred = 1.01\n",
        "\n",
        "print('Generating validation set...')\n",
        "\n",
        "params = {'batch_size': 1, 'shuffle': True, 'num_workers': 2}\n",
        "\n",
        "val_loader = Gleason2019('val', train_imgs, labels_final, split = (0.8, 0.2), classes,\n",
        "                     crop_dim=(dim, dim), samples=samples_per_image_val)\n",
        "val_generator = DataLoader(val_loader, **params)\n",
        "  \n",
        "for epoch in range(1, epochs + 1):\n",
        "  print('generating train set...')\n",
        "  \n",
        "  train_loader = Gleason2019('train', train_imgs, labels_final, split = (0.8, 0.2), classes,\n",
        "                         crop_dim=(dim, dim), samples=samples_per_image_train)\n",
        "  training_generator = DataLoader(train_loader, **params)\n",
        "  # Train Stats\n",
        "  train_name = 'train_{}.csv'.format(datestr())\n",
        "  val_name =  'val_{}.csv'.format(datestr())\n",
        "  train_f = open(os.path.join(root_path, train_name ), 'w')\n",
        "  val_f = open(os.path.join(root_path, val_name ), 'w')\n",
        "  \n",
        "  save_path = 'checkpoints/Unet'+'_base_{}'.format(datestr())\n",
        "  if os.path.exists(save_path):\n",
        "    shutil.rmtree(save_path)\n",
        "    os.mkdir(save_path)\n",
        "  else:\n",
        "    os.makedirs(save_path)\n",
        "  \n",
        "  for subepoch in range(1, subepochs + 1):\n",
        "    id = str(epoch) + '_' + str(subepoch)\n",
        "    train(subepoch, model, training_generator, optimizer, criterion, train_f)\n",
        "    dice_loss = evaluate(subepoch, model, val_generator, optimizer, criterion, val_f)\n",
        "\n",
        "    is_best = False\n",
        "    if dice_loss < best_pred:\n",
        "        is_best = True\n",
        "        best_pred = dice_loss\n",
        "        save_checkpoint({'epoch': epoch,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'best_prec1': best_pred},\n",
        "                        is_best, save_path, 'UNET_'+id , 'checkpoint.pth.tar')\n",
        "    else:\n",
        "        save_checkpoint({'epoch': epoch,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'best_prec1': best_pred},\n",
        "                        is_best, save_path, 'UNET_'+id , '_LAST.pth.tar')\n",
        "  \n",
        "train_f.close()\n",
        "val_f.close()\n",
        "print(\"THE END!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating validation set...\n",
            "Total val data to generate samples: 48\n",
            "generating train set...\n",
            "Total train data to generate samples: 195\n",
            "\n",
            " Train Epoch 1 Summary:  \t Dice Loss: 0.8793\t AVG Dice Coeff: 12.0717\n",
            "\n",
            " Validation Epoch 1 Summary:  \t Dice Loss: 0.8771\t AVG Dice Coeff: 12.2859\n",
            "\n",
            " Train Epoch 2 Summary:  \t Dice Loss: 0.8792\t AVG Dice Coeff: 12.0778\n",
            "\n",
            " Validation Epoch 2 Summary:  \t Dice Loss: 0.8771\t AVG Dice Coeff: 12.2895\n",
            "\n",
            " Train Epoch 3 Summary:  \t Dice Loss: 0.8792\t AVG Dice Coeff: 12.0840\n",
            "\n",
            " Validation Epoch 3 Summary:  \t Dice Loss: 0.8770\t AVG Dice Coeff: 12.3007\n",
            "\n",
            " Train Epoch 4 Summary:  \t Dice Loss: 0.8791\t AVG Dice Coeff: 12.0901\n",
            "\n",
            " Validation Epoch 4 Summary:  \t Dice Loss: 0.8769\t AVG Dice Coeff: 12.3089\n",
            "\n",
            " Train Epoch 5 Summary:  \t Dice Loss: 0.8790\t AVG Dice Coeff: 12.0963\n",
            "\n",
            " Validation Epoch 5 Summary:  \t Dice Loss: 0.8769\t AVG Dice Coeff: 12.3117\n",
            "\n",
            " Train Epoch 6 Summary:  \t Dice Loss: 0.8790\t AVG Dice Coeff: 12.1024\n",
            "\n",
            " Validation Epoch 6 Summary:  \t Dice Loss: 0.8768\t AVG Dice Coeff: 12.3227\n",
            "\n",
            " Train Epoch 7 Summary:  \t Dice Loss: 0.8789\t AVG Dice Coeff: 12.1085\n",
            "\n",
            " Validation Epoch 7 Summary:  \t Dice Loss: 0.8768\t AVG Dice Coeff: 12.3236\n",
            "\n",
            " Train Epoch 8 Summary:  \t Dice Loss: 0.8789\t AVG Dice Coeff: 12.1145\n",
            "\n",
            " Validation Epoch 8 Summary:  \t Dice Loss: 0.8766\t AVG Dice Coeff: 12.3365\n",
            "\n",
            " Train Epoch 9 Summary:  \t Dice Loss: 0.8788\t AVG Dice Coeff: 12.1206\n",
            "\n",
            " Validation Epoch 9 Summary:  \t Dice Loss: 0.8766\t AVG Dice Coeff: 12.3395\n",
            "\n",
            " Train Epoch 10 Summary:  \t Dice Loss: 0.8787\t AVG Dice Coeff: 12.1266\n",
            "\n",
            " Validation Epoch 10 Summary:  \t Dice Loss: 0.8765\t AVG Dice Coeff: 12.3543\n",
            "\n",
            " Train Epoch 11 Summary:  \t Dice Loss: 0.8787\t AVG Dice Coeff: 12.1326\n",
            "\n",
            " Validation Epoch 11 Summary:  \t Dice Loss: 0.8764\t AVG Dice Coeff: 12.3634\n",
            "\n",
            " Train Epoch 12 Summary:  \t Dice Loss: 0.8786\t AVG Dice Coeff: 12.1385\n",
            "\n",
            " Validation Epoch 12 Summary:  \t Dice Loss: 0.8763\t AVG Dice Coeff: 12.3662\n",
            "\n",
            " Train Epoch 13 Summary:  \t Dice Loss: 0.8786\t AVG Dice Coeff: 12.1445\n",
            "\n",
            " Validation Epoch 13 Summary:  \t Dice Loss: 0.8763\t AVG Dice Coeff: 12.3689\n",
            "\n",
            " Train Epoch 14 Summary:  \t Dice Loss: 0.8785\t AVG Dice Coeff: 12.1504\n",
            "\n",
            " Validation Epoch 14 Summary:  \t Dice Loss: 0.8763\t AVG Dice Coeff: 12.3659\n",
            "\n",
            " Train Epoch 15 Summary:  \t Dice Loss: 0.8784\t AVG Dice Coeff: 12.1564\n",
            "\n",
            " Validation Epoch 15 Summary:  \t Dice Loss: 0.8762\t AVG Dice Coeff: 12.3827\n",
            "\n",
            " Train Epoch 16 Summary:  \t Dice Loss: 0.8784\t AVG Dice Coeff: 12.1624\n",
            "\n",
            " Validation Epoch 16 Summary:  \t Dice Loss: 0.8761\t AVG Dice Coeff: 12.3886\n",
            "\n",
            " Train Epoch 17 Summary:  \t Dice Loss: 0.8783\t AVG Dice Coeff: 12.1683\n",
            "\n",
            " Validation Epoch 17 Summary:  \t Dice Loss: 0.8761\t AVG Dice Coeff: 12.3896\n",
            "\n",
            " Train Epoch 18 Summary:  \t Dice Loss: 0.8783\t AVG Dice Coeff: 12.1743\n",
            "\n",
            " Validation Epoch 18 Summary:  \t Dice Loss: 0.8760\t AVG Dice Coeff: 12.4002\n",
            "\n",
            " Train Epoch 19 Summary:  \t Dice Loss: 0.8782\t AVG Dice Coeff: 12.1802\n",
            "\n",
            " Validation Epoch 19 Summary:  \t Dice Loss: 0.8759\t AVG Dice Coeff: 12.4077\n",
            "\n",
            " Train Epoch 20 Summary:  \t Dice Loss: 0.8781\t AVG Dice Coeff: 12.1862\n",
            "\n",
            " Validation Epoch 20 Summary:  \t Dice Loss: 0.8758\t AVG Dice Coeff: 12.4183\n",
            "\n",
            " Train Epoch 21 Summary:  \t Dice Loss: 0.8781\t AVG Dice Coeff: 12.1921\n",
            "\n",
            " Validation Epoch 21 Summary:  \t Dice Loss: 0.8756\t AVG Dice Coeff: 12.4355\n",
            "\n",
            " Train Epoch 22 Summary:  \t Dice Loss: 0.8780\t AVG Dice Coeff: 12.1979\n",
            "\n",
            " Validation Epoch 22 Summary:  \t Dice Loss: 0.8756\t AVG Dice Coeff: 12.4354\n",
            "\n",
            " Train Epoch 23 Summary:  \t Dice Loss: 0.8780\t AVG Dice Coeff: 12.2037\n",
            "\n",
            " Validation Epoch 23 Summary:  \t Dice Loss: 0.8756\t AVG Dice Coeff: 12.4359\n",
            "\n",
            " Train Epoch 24 Summary:  \t Dice Loss: 0.8779\t AVG Dice Coeff: 12.2094\n",
            "\n",
            " Validation Epoch 24 Summary:  \t Dice Loss: 0.8756\t AVG Dice Coeff: 12.4422\n",
            "\n",
            " Train Epoch 25 Summary:  \t Dice Loss: 0.8778\t AVG Dice Coeff: 12.2151\n",
            "\n",
            " Validation Epoch 25 Summary:  \t Dice Loss: 0.8756\t AVG Dice Coeff: 12.4445\n",
            "\n",
            " Train Epoch 26 Summary:  \t Dice Loss: 0.8778\t AVG Dice Coeff: 12.2206\n",
            "\n",
            " Validation Epoch 26 Summary:  \t Dice Loss: 0.8754\t AVG Dice Coeff: 12.4640\n",
            "\n",
            " Train Epoch 27 Summary:  \t Dice Loss: 0.8777\t AVG Dice Coeff: 12.2261\n",
            "\n",
            " Validation Epoch 27 Summary:  \t Dice Loss: 0.8754\t AVG Dice Coeff: 12.4564\n",
            "\n",
            " Train Epoch 28 Summary:  \t Dice Loss: 0.8777\t AVG Dice Coeff: 12.2315\n",
            "\n",
            " Validation Epoch 28 Summary:  \t Dice Loss: 0.8754\t AVG Dice Coeff: 12.4606\n",
            "\n",
            " Train Epoch 29 Summary:  \t Dice Loss: 0.8776\t AVG Dice Coeff: 12.2368\n",
            "\n",
            " Validation Epoch 29 Summary:  \t Dice Loss: 0.8753\t AVG Dice Coeff: 12.4746\n",
            "\n",
            " Train Epoch 30 Summary:  \t Dice Loss: 0.8776\t AVG Dice Coeff: 12.2421\n",
            "\n",
            " Validation Epoch 30 Summary:  \t Dice Loss: 0.8752\t AVG Dice Coeff: 12.4812\n",
            "\n",
            " Train Epoch 31 Summary:  \t Dice Loss: 0.8775\t AVG Dice Coeff: 12.2473\n",
            "\n",
            " Validation Epoch 31 Summary:  \t Dice Loss: 0.8752\t AVG Dice Coeff: 12.4824\n",
            "\n",
            " Train Epoch 32 Summary:  \t Dice Loss: 0.8775\t AVG Dice Coeff: 12.2524\n",
            "\n",
            " Validation Epoch 32 Summary:  \t Dice Loss: 0.8751\t AVG Dice Coeff: 12.4880\n",
            "\n",
            " Train Epoch 33 Summary:  \t Dice Loss: 0.8774\t AVG Dice Coeff: 12.2575\n",
            "\n",
            " Validation Epoch 33 Summary:  \t Dice Loss: 0.8750\t AVG Dice Coeff: 12.4961\n",
            "\n",
            " Train Epoch 34 Summary:  \t Dice Loss: 0.8774\t AVG Dice Coeff: 12.2625\n",
            "\n",
            " Validation Epoch 34 Summary:  \t Dice Loss: 0.8750\t AVG Dice Coeff: 12.5037\n",
            "\n",
            " Train Epoch 35 Summary:  \t Dice Loss: 0.8773\t AVG Dice Coeff: 12.2674\n",
            "\n",
            " Validation Epoch 35 Summary:  \t Dice Loss: 0.8749\t AVG Dice Coeff: 12.5123\n",
            "\n",
            " Train Epoch 36 Summary:  \t Dice Loss: 0.8773\t AVG Dice Coeff: 12.2723\n",
            "\n",
            " Validation Epoch 36 Summary:  \t Dice Loss: 0.8749\t AVG Dice Coeff: 12.5050\n",
            "\n",
            " Train Epoch 37 Summary:  \t Dice Loss: 0.8772\t AVG Dice Coeff: 12.2771\n",
            "\n",
            " Validation Epoch 37 Summary:  \t Dice Loss: 0.8749\t AVG Dice Coeff: 12.5114\n",
            "\n",
            " Train Epoch 38 Summary:  \t Dice Loss: 0.8772\t AVG Dice Coeff: 12.2819\n",
            "\n",
            " Validation Epoch 38 Summary:  \t Dice Loss: 0.8747\t AVG Dice Coeff: 12.5278\n",
            "\n",
            " Train Epoch 39 Summary:  \t Dice Loss: 0.8771\t AVG Dice Coeff: 12.2866\n",
            "\n",
            " Validation Epoch 39 Summary:  \t Dice Loss: 0.8748\t AVG Dice Coeff: 12.5238\n",
            "\n",
            " Train Epoch 40 Summary:  \t Dice Loss: 0.8771\t AVG Dice Coeff: 12.2912\n",
            "\n",
            " Validation Epoch 40 Summary:  \t Dice Loss: 0.8746\t AVG Dice Coeff: 12.5358\n",
            "\n",
            " Train Epoch 41 Summary:  \t Dice Loss: 0.8770\t AVG Dice Coeff: 12.2958\n",
            "\n",
            " Validation Epoch 41 Summary:  \t Dice Loss: 0.8747\t AVG Dice Coeff: 12.5330\n",
            "\n",
            " Train Epoch 42 Summary:  \t Dice Loss: 0.8770\t AVG Dice Coeff: 12.3004\n",
            "\n",
            " Validation Epoch 42 Summary:  \t Dice Loss: 0.8746\t AVG Dice Coeff: 12.5422\n",
            "\n",
            " Train Epoch 43 Summary:  \t Dice Loss: 0.8770\t AVG Dice Coeff: 12.3049\n",
            "\n",
            " Validation Epoch 43 Summary:  \t Dice Loss: 0.8744\t AVG Dice Coeff: 12.5617\n",
            "\n",
            " Train Epoch 44 Summary:  \t Dice Loss: 0.8769\t AVG Dice Coeff: 12.3093\n",
            "\n",
            " Validation Epoch 44 Summary:  \t Dice Loss: 0.8745\t AVG Dice Coeff: 12.5500\n",
            "\n",
            " Train Epoch 45 Summary:  \t Dice Loss: 0.8769\t AVG Dice Coeff: 12.3138\n",
            "\n",
            " Validation Epoch 45 Summary:  \t Dice Loss: 0.8745\t AVG Dice Coeff: 12.5467\n",
            "\n",
            " Train Epoch 46 Summary:  \t Dice Loss: 0.8768\t AVG Dice Coeff: 12.3182\n",
            "\n",
            " Validation Epoch 46 Summary:  \t Dice Loss: 0.8743\t AVG Dice Coeff: 12.5721\n",
            "\n",
            " Train Epoch 47 Summary:  \t Dice Loss: 0.8768\t AVG Dice Coeff: 12.3225\n",
            "\n",
            " Validation Epoch 47 Summary:  \t Dice Loss: 0.8745\t AVG Dice Coeff: 12.5472\n",
            "\n",
            " Train Epoch 48 Summary:  \t Dice Loss: 0.8767\t AVG Dice Coeff: 12.3268\n",
            "\n",
            " Validation Epoch 48 Summary:  \t Dice Loss: 0.8743\t AVG Dice Coeff: 12.5666\n",
            "\n",
            " Train Epoch 49 Summary:  \t Dice Loss: 0.8767\t AVG Dice Coeff: 12.3311\n",
            "\n",
            " Validation Epoch 49 Summary:  \t Dice Loss: 0.8744\t AVG Dice Coeff: 12.5650\n",
            "\n",
            " Train Epoch 50 Summary:  \t Dice Loss: 0.8766\t AVG Dice Coeff: 12.3353\n",
            "\n",
            " Validation Epoch 50 Summary:  \t Dice Loss: 0.8743\t AVG Dice Coeff: 12.5678\n",
            "generating train set...\n",
            "Total train data to generate samples: 195\n",
            "\n",
            " Train Epoch 1 Summary:  \t Dice Loss: 0.8755\t AVG Dice Coeff: 12.4496\n",
            "\n",
            " Validation Epoch 1 Summary:  \t Dice Loss: 0.8743\t AVG Dice Coeff: 12.5730\n",
            "\n",
            " Train Epoch 2 Summary:  \t Dice Loss: 0.8755\t AVG Dice Coeff: 12.4538\n",
            "\n",
            " Validation Epoch 2 Summary:  \t Dice Loss: 0.8741\t AVG Dice Coeff: 12.5896\n",
            "\n",
            " Train Epoch 3 Summary:  \t Dice Loss: 0.8754\t AVG Dice Coeff: 12.4579\n",
            "\n",
            " Validation Epoch 3 Summary:  \t Dice Loss: 0.8742\t AVG Dice Coeff: 12.5800\n",
            "\n",
            " Train Epoch 4 Summary:  \t Dice Loss: 0.8754\t AVG Dice Coeff: 12.4620\n",
            "\n",
            " Validation Epoch 4 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.5997\n",
            "\n",
            " Train Epoch 5 Summary:  \t Dice Loss: 0.8753\t AVG Dice Coeff: 12.4661\n",
            "\n",
            " Validation Epoch 5 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.5988\n",
            "\n",
            " Train Epoch 6 Summary:  \t Dice Loss: 0.8753\t AVG Dice Coeff: 12.4701\n",
            "\n",
            " Validation Epoch 6 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.5987\n",
            "\n",
            " Train Epoch 7 Summary:  \t Dice Loss: 0.8753\t AVG Dice Coeff: 12.4741\n",
            "\n",
            " Validation Epoch 7 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.5975\n",
            "\n",
            " Train Epoch 8 Summary:  \t Dice Loss: 0.8752\t AVG Dice Coeff: 12.4781\n",
            "\n",
            " Validation Epoch 8 Summary:  \t Dice Loss: 0.8739\t AVG Dice Coeff: 12.6076\n",
            "\n",
            " Train Epoch 9 Summary:  \t Dice Loss: 0.8752\t AVG Dice Coeff: 12.4820\n",
            "\n",
            " Validation Epoch 9 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.6025\n",
            "\n",
            " Train Epoch 10 Summary:  \t Dice Loss: 0.8751\t AVG Dice Coeff: 12.4858\n",
            "\n",
            " Validation Epoch 10 Summary:  \t Dice Loss: 0.8738\t AVG Dice Coeff: 12.6205\n",
            "\n",
            " Train Epoch 11 Summary:  \t Dice Loss: 0.8751\t AVG Dice Coeff: 12.4897\n",
            "\n",
            " Validation Epoch 11 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.6010\n",
            "\n",
            " Train Epoch 12 Summary:  \t Dice Loss: 0.8751\t AVG Dice Coeff: 12.4935\n",
            "\n",
            " Validation Epoch 12 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.6043\n",
            "\n",
            " Train Epoch 13 Summary:  \t Dice Loss: 0.8750\t AVG Dice Coeff: 12.4973\n",
            "\n",
            " Validation Epoch 13 Summary:  \t Dice Loss: 0.8736\t AVG Dice Coeff: 12.6414\n",
            "\n",
            " Train Epoch 14 Summary:  \t Dice Loss: 0.8750\t AVG Dice Coeff: 12.5010\n",
            "\n",
            " Validation Epoch 14 Summary:  \t Dice Loss: 0.8737\t AVG Dice Coeff: 12.6315\n",
            "\n",
            " Train Epoch 15 Summary:  \t Dice Loss: 0.8750\t AVG Dice Coeff: 12.5048\n",
            "\n",
            " Validation Epoch 15 Summary:  \t Dice Loss: 0.8737\t AVG Dice Coeff: 12.6290\n",
            "\n",
            " Train Epoch 16 Summary:  \t Dice Loss: 0.8749\t AVG Dice Coeff: 12.5085\n",
            "\n",
            " Validation Epoch 16 Summary:  \t Dice Loss: 0.8736\t AVG Dice Coeff: 12.6434\n",
            "\n",
            " Train Epoch 17 Summary:  \t Dice Loss: 0.8749\t AVG Dice Coeff: 12.5121\n",
            "\n",
            " Validation Epoch 17 Summary:  \t Dice Loss: 0.8735\t AVG Dice Coeff: 12.6466\n",
            "\n",
            " Train Epoch 18 Summary:  \t Dice Loss: 0.8748\t AVG Dice Coeff: 12.5158\n",
            "\n",
            " Validation Epoch 18 Summary:  \t Dice Loss: 0.8737\t AVG Dice Coeff: 12.6269\n",
            "\n",
            " Train Epoch 19 Summary:  \t Dice Loss: 0.8748\t AVG Dice Coeff: 12.5194\n",
            "\n",
            " Validation Epoch 19 Summary:  \t Dice Loss: 0.8736\t AVG Dice Coeff: 12.6370\n",
            "\n",
            " Train Epoch 20 Summary:  \t Dice Loss: 0.8748\t AVG Dice Coeff: 12.5230\n",
            "\n",
            " Validation Epoch 20 Summary:  \t Dice Loss: 0.8737\t AVG Dice Coeff: 12.6338\n",
            "\n",
            " Train Epoch 21 Summary:  \t Dice Loss: 0.8747\t AVG Dice Coeff: 12.5266\n",
            "\n",
            " Validation Epoch 21 Summary:  \t Dice Loss: 0.8736\t AVG Dice Coeff: 12.6441\n",
            "\n",
            " Train Epoch 22 Summary:  \t Dice Loss: 0.8747\t AVG Dice Coeff: 12.5301\n",
            "\n",
            " Validation Epoch 22 Summary:  \t Dice Loss: 0.8735\t AVG Dice Coeff: 12.6508\n",
            "\n",
            " Train Epoch 23 Summary:  \t Dice Loss: 0.8747\t AVG Dice Coeff: 12.5336\n",
            "\n",
            " Validation Epoch 23 Summary:  \t Dice Loss: 0.8733\t AVG Dice Coeff: 12.6652\n",
            "\n",
            " Train Epoch 24 Summary:  \t Dice Loss: 0.8746\t AVG Dice Coeff: 12.5371\n",
            "\n",
            " Validation Epoch 24 Summary:  \t Dice Loss: 0.8732\t AVG Dice Coeff: 12.6842\n",
            "\n",
            " Train Epoch 25 Summary:  \t Dice Loss: 0.8746\t AVG Dice Coeff: 12.5406\n",
            "\n",
            " Validation Epoch 25 Summary:  \t Dice Loss: 0.8734\t AVG Dice Coeff: 12.6588\n",
            "\n",
            " Train Epoch 26 Summary:  \t Dice Loss: 0.8746\t AVG Dice Coeff: 12.5440\n",
            "\n",
            " Validation Epoch 26 Summary:  \t Dice Loss: 0.8733\t AVG Dice Coeff: 12.6748\n",
            "\n",
            " Train Epoch 27 Summary:  \t Dice Loss: 0.8745\t AVG Dice Coeff: 12.5475\n",
            "\n",
            " Validation Epoch 27 Summary:  \t Dice Loss: 0.8731\t AVG Dice Coeff: 12.6852\n",
            "\n",
            " Train Epoch 28 Summary:  \t Dice Loss: 0.8745\t AVG Dice Coeff: 12.5509\n",
            "\n",
            " Validation Epoch 28 Summary:  \t Dice Loss: 0.8732\t AVG Dice Coeff: 12.6846\n",
            "\n",
            " Train Epoch 29 Summary:  \t Dice Loss: 0.8745\t AVG Dice Coeff: 12.5542\n",
            "\n",
            " Validation Epoch 29 Summary:  \t Dice Loss: 0.8732\t AVG Dice Coeff: 12.6808\n",
            "\n",
            " Train Epoch 30 Summary:  \t Dice Loss: 0.8744\t AVG Dice Coeff: 12.5576\n",
            "\n",
            " Validation Epoch 30 Summary:  \t Dice Loss: 0.8731\t AVG Dice Coeff: 12.6889\n",
            "\n",
            " Train Epoch 31 Summary:  \t Dice Loss: 0.8744\t AVG Dice Coeff: 12.5609\n",
            "\n",
            " Validation Epoch 31 Summary:  \t Dice Loss: 0.8730\t AVG Dice Coeff: 12.7045\n",
            "\n",
            " Train Epoch 32 Summary:  \t Dice Loss: 0.8744\t AVG Dice Coeff: 12.5642\n",
            "\n",
            " Validation Epoch 32 Summary:  \t Dice Loss: 0.8729\t AVG Dice Coeff: 12.7067\n",
            "\n",
            " Train Epoch 33 Summary:  \t Dice Loss: 0.8743\t AVG Dice Coeff: 12.5675\n",
            "\n",
            " Validation Epoch 33 Summary:  \t Dice Loss: 0.8731\t AVG Dice Coeff: 12.6917\n",
            "\n",
            " Train Epoch 34 Summary:  \t Dice Loss: 0.8743\t AVG Dice Coeff: 12.5708\n",
            "\n",
            " Validation Epoch 34 Summary:  \t Dice Loss: 0.8729\t AVG Dice Coeff: 12.7070\n",
            "\n",
            " Train Epoch 35 Summary:  \t Dice Loss: 0.8743\t AVG Dice Coeff: 12.5740\n",
            "\n",
            " Validation Epoch 35 Summary:  \t Dice Loss: 0.8729\t AVG Dice Coeff: 12.7085\n",
            "\n",
            " Train Epoch 36 Summary:  \t Dice Loss: 0.8742\t AVG Dice Coeff: 12.5772\n",
            "\n",
            " Validation Epoch 36 Summary:  \t Dice Loss: 0.8731\t AVG Dice Coeff: 12.6884\n",
            "\n",
            " Train Epoch 37 Summary:  \t Dice Loss: 0.8742\t AVG Dice Coeff: 12.5804\n",
            "\n",
            " Validation Epoch 37 Summary:  \t Dice Loss: 0.8729\t AVG Dice Coeff: 12.7050\n",
            "\n",
            " Train Epoch 38 Summary:  \t Dice Loss: 0.8742\t AVG Dice Coeff: 12.5836\n",
            "\n",
            " Validation Epoch 38 Summary:  \t Dice Loss: 0.8728\t AVG Dice Coeff: 12.7218\n",
            "\n",
            " Train Epoch 39 Summary:  \t Dice Loss: 0.8741\t AVG Dice Coeff: 12.5868\n",
            "\n",
            " Validation Epoch 39 Summary:  \t Dice Loss: 0.8727\t AVG Dice Coeff: 12.7310\n",
            "\n",
            " Train Epoch 40 Summary:  \t Dice Loss: 0.8741\t AVG Dice Coeff: 12.5899\n",
            "\n",
            " Validation Epoch 40 Summary:  \t Dice Loss: 0.8727\t AVG Dice Coeff: 12.7300\n",
            "\n",
            " Train Epoch 41 Summary:  \t Dice Loss: 0.8741\t AVG Dice Coeff: 12.5930\n",
            "\n",
            " Validation Epoch 41 Summary:  \t Dice Loss: 0.8728\t AVG Dice Coeff: 12.7172\n",
            "\n",
            " Train Epoch 42 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.5961\n",
            "\n",
            " Validation Epoch 42 Summary:  \t Dice Loss: 0.8727\t AVG Dice Coeff: 12.7274\n",
            "\n",
            " Train Epoch 43 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.5992\n",
            "\n",
            " Validation Epoch 43 Summary:  \t Dice Loss: 0.8727\t AVG Dice Coeff: 12.7345\n",
            "\n",
            " Train Epoch 44 Summary:  \t Dice Loss: 0.8740\t AVG Dice Coeff: 12.6022\n",
            "\n",
            " Validation Epoch 44 Summary:  \t Dice Loss: 0.8726\t AVG Dice Coeff: 12.7432\n",
            "\n",
            " Train Epoch 45 Summary:  \t Dice Loss: 0.8739\t AVG Dice Coeff: 12.6052\n",
            "\n",
            " Validation Epoch 45 Summary:  \t Dice Loss: 0.8726\t AVG Dice Coeff: 12.7449\n",
            "\n",
            " Train Epoch 46 Summary:  \t Dice Loss: 0.8739\t AVG Dice Coeff: 12.6082\n",
            "\n",
            " Validation Epoch 46 Summary:  \t Dice Loss: 0.8726\t AVG Dice Coeff: 12.7446\n",
            "\n",
            " Train Epoch 47 Summary:  \t Dice Loss: 0.8739\t AVG Dice Coeff: 12.6112\n",
            "\n",
            " Validation Epoch 47 Summary:  \t Dice Loss: 0.8725\t AVG Dice Coeff: 12.7502\n",
            "\n",
            " Train Epoch 48 Summary:  \t Dice Loss: 0.8739\t AVG Dice Coeff: 12.6142\n",
            "\n",
            " Validation Epoch 48 Summary:  \t Dice Loss: 0.8725\t AVG Dice Coeff: 12.7508\n",
            "\n",
            " Train Epoch 49 Summary:  \t Dice Loss: 0.8738\t AVG Dice Coeff: 12.6171\n",
            "\n",
            " Validation Epoch 49 Summary:  \t Dice Loss: 0.8726\t AVG Dice Coeff: 12.7364\n",
            "\n",
            " Train Epoch 50 Summary:  \t Dice Loss: 0.8738\t AVG Dice Coeff: 12.6201\n",
            "\n",
            " Validation Epoch 50 Summary:  \t Dice Loss: 0.8726\t AVG Dice Coeff: 12.7409\n",
            "generating train set...\n",
            "Total train data to generate samples: 195\n",
            "\n",
            " Train Epoch 1 Summary:  \t Dice Loss: 0.8737\t AVG Dice Coeff: 12.6288\n",
            "\n",
            " Validation Epoch 1 Summary:  \t Dice Loss: 0.8724\t AVG Dice Coeff: 12.7620\n",
            "\n",
            " Train Epoch 2 Summary:  \t Dice Loss: 0.8737\t AVG Dice Coeff: 12.6320\n",
            "\n",
            " Validation Epoch 2 Summary:  \t Dice Loss: 0.8722\t AVG Dice Coeff: 12.7812\n",
            "\n",
            " Train Epoch 3 Summary:  \t Dice Loss: 0.8736\t AVG Dice Coeff: 12.6351\n",
            "\n",
            " Validation Epoch 3 Summary:  \t Dice Loss: 0.8727\t AVG Dice Coeff: 12.7334\n",
            "\n",
            " Train Epoch 4 Summary:  \t Dice Loss: 0.8736\t AVG Dice Coeff: 12.6382\n",
            "\n",
            " Validation Epoch 4 Summary:  \t Dice Loss: 0.8721\t AVG Dice Coeff: 12.7880\n",
            "\n",
            " Train Epoch 5 Summary:  \t Dice Loss: 0.8736\t AVG Dice Coeff: 12.6413\n",
            "\n",
            " Validation Epoch 5 Summary:  \t Dice Loss: 0.8724\t AVG Dice Coeff: 12.7592\n",
            "\n",
            " Train Epoch 6 Summary:  \t Dice Loss: 0.8736\t AVG Dice Coeff: 12.6444\n",
            "\n",
            " Validation Epoch 6 Summary:  \t Dice Loss: 0.8721\t AVG Dice Coeff: 12.7906\n",
            "\n",
            " Train Epoch 7 Summary:  \t Dice Loss: 0.8735\t AVG Dice Coeff: 12.6474\n",
            "\n",
            " Validation Epoch 7 Summary:  \t Dice Loss: 0.8721\t AVG Dice Coeff: 12.7947\n",
            "\n",
            " Train Epoch 8 Summary:  \t Dice Loss: 0.8735\t AVG Dice Coeff: 12.6504\n",
            "\n",
            " Validation Epoch 8 Summary:  \t Dice Loss: 0.8725\t AVG Dice Coeff: 12.7479\n",
            "\n",
            " Train Epoch 9 Summary:  \t Dice Loss: 0.8735\t AVG Dice Coeff: 12.6533\n",
            "\n",
            " Validation Epoch 9 Summary:  \t Dice Loss: 0.8722\t AVG Dice Coeff: 12.7828\n",
            "\n",
            " Train Epoch 10 Summary:  \t Dice Loss: 0.8734\t AVG Dice Coeff: 12.6563\n",
            "\n",
            " Validation Epoch 10 Summary:  \t Dice Loss: 0.8721\t AVG Dice Coeff: 12.7861\n",
            "\n",
            " Train Epoch 11 Summary:  \t Dice Loss: 0.8734\t AVG Dice Coeff: 12.6592\n",
            "\n",
            " Validation Epoch 11 Summary:  \t Dice Loss: 0.8721\t AVG Dice Coeff: 12.7929\n",
            "\n",
            " Train Epoch 12 Summary:  \t Dice Loss: 0.8734\t AVG Dice Coeff: 12.6621\n",
            "\n",
            " Validation Epoch 12 Summary:  \t Dice Loss: 0.8719\t AVG Dice Coeff: 12.8083\n",
            "\n",
            " Train Epoch 13 Summary:  \t Dice Loss: 0.8734\t AVG Dice Coeff: 12.6650\n",
            "\n",
            " Validation Epoch 13 Summary:  \t Dice Loss: 0.8720\t AVG Dice Coeff: 12.8036\n",
            "\n",
            " Train Epoch 14 Summary:  \t Dice Loss: 0.8733\t AVG Dice Coeff: 12.6678\n",
            "\n",
            " Validation Epoch 14 Summary:  \t Dice Loss: 0.8718\t AVG Dice Coeff: 12.8164\n",
            "\n",
            " Train Epoch 15 Summary:  \t Dice Loss: 0.8733\t AVG Dice Coeff: 12.6707\n",
            "\n",
            " Validation Epoch 15 Summary:  \t Dice Loss: 0.8720\t AVG Dice Coeff: 12.8048\n",
            "\n",
            " Train Epoch 16 Summary:  \t Dice Loss: 0.8733\t AVG Dice Coeff: 12.6735\n",
            "\n",
            " Validation Epoch 16 Summary:  \t Dice Loss: 0.8721\t AVG Dice Coeff: 12.7930\n",
            "\n",
            " Train Epoch 17 Summary:  \t Dice Loss: 0.8732\t AVG Dice Coeff: 12.6763\n",
            "\n",
            " Validation Epoch 17 Summary:  \t Dice Loss: 0.8720\t AVG Dice Coeff: 12.8049\n",
            "\n",
            " Train Epoch 18 Summary:  \t Dice Loss: 0.8732\t AVG Dice Coeff: 12.6790\n",
            "\n",
            " Validation Epoch 18 Summary:  \t Dice Loss: 0.8718\t AVG Dice Coeff: 12.8178\n",
            "\n",
            " Train Epoch 19 Summary:  \t Dice Loss: 0.8732\t AVG Dice Coeff: 12.6818\n",
            "\n",
            " Validation Epoch 19 Summary:  \t Dice Loss: 0.8718\t AVG Dice Coeff: 12.8167\n",
            "\n",
            " Train Epoch 20 Summary:  \t Dice Loss: 0.8732\t AVG Dice Coeff: 12.6845\n",
            "\n",
            " Validation Epoch 20 Summary:  \t Dice Loss: 0.8718\t AVG Dice Coeff: 12.8178\n",
            "\n",
            " Train Epoch 21 Summary:  \t Dice Loss: 0.8731\t AVG Dice Coeff: 12.6872\n",
            "\n",
            " Validation Epoch 21 Summary:  \t Dice Loss: 0.8718\t AVG Dice Coeff: 12.8217\n",
            "\n",
            " Train Epoch 22 Summary:  \t Dice Loss: 0.8731\t AVG Dice Coeff: 12.6899\n",
            "\n",
            " Validation Epoch 22 Summary:  \t Dice Loss: 0.8717\t AVG Dice Coeff: 12.8333\n",
            "\n",
            " Train Epoch 23 Summary:  \t Dice Loss: 0.8731\t AVG Dice Coeff: 12.6925\n",
            "\n",
            " Validation Epoch 23 Summary:  \t Dice Loss: 0.8716\t AVG Dice Coeff: 12.8380\n",
            "\n",
            " Train Epoch 24 Summary:  \t Dice Loss: 0.8730\t AVG Dice Coeff: 12.6952\n",
            "\n",
            " Validation Epoch 24 Summary:  \t Dice Loss: 0.8716\t AVG Dice Coeff: 12.8421\n",
            "\n",
            " Train Epoch 25 Summary:  \t Dice Loss: 0.8730\t AVG Dice Coeff: 12.6978\n",
            "\n",
            " Validation Epoch 25 Summary:  \t Dice Loss: 0.8718\t AVG Dice Coeff: 12.8180\n",
            "\n",
            " Train Epoch 26 Summary:  \t Dice Loss: 0.8730\t AVG Dice Coeff: 12.7004\n",
            "\n",
            " Validation Epoch 26 Summary:  \t Dice Loss: 0.8715\t AVG Dice Coeff: 12.8540\n",
            "\n",
            " Train Epoch 27 Summary:  \t Dice Loss: 0.8730\t AVG Dice Coeff: 12.7030\n",
            "\n",
            " Validation Epoch 27 Summary:  \t Dice Loss: 0.8717\t AVG Dice Coeff: 12.8303\n",
            "\n",
            " Train Epoch 28 Summary:  \t Dice Loss: 0.8729\t AVG Dice Coeff: 12.7056\n",
            "\n",
            " Validation Epoch 28 Summary:  \t Dice Loss: 0.8716\t AVG Dice Coeff: 12.8378\n",
            "\n",
            " Train Epoch 29 Summary:  \t Dice Loss: 0.8729\t AVG Dice Coeff: 12.7082\n",
            "\n",
            " Validation Epoch 29 Summary:  \t Dice Loss: 0.8714\t AVG Dice Coeff: 12.8556\n",
            "\n",
            " Train Epoch 30 Summary:  \t Dice Loss: 0.8729\t AVG Dice Coeff: 12.7107\n",
            "\n",
            " Validation Epoch 30 Summary:  \t Dice Loss: 0.8713\t AVG Dice Coeff: 12.8661\n",
            "\n",
            " Train Epoch 31 Summary:  \t Dice Loss: 0.8729\t AVG Dice Coeff: 12.7132\n",
            "\n",
            " Validation Epoch 31 Summary:  \t Dice Loss: 0.8715\t AVG Dice Coeff: 12.8458\n",
            "\n",
            " Train Epoch 32 Summary:  \t Dice Loss: 0.8728\t AVG Dice Coeff: 12.7157\n",
            "\n",
            " Validation Epoch 32 Summary:  \t Dice Loss: 0.8714\t AVG Dice Coeff: 12.8573\n",
            "\n",
            " Train Epoch 33 Summary:  \t Dice Loss: 0.8728\t AVG Dice Coeff: 12.7182\n",
            "\n",
            " Validation Epoch 33 Summary:  \t Dice Loss: 0.8714\t AVG Dice Coeff: 12.8595\n",
            "\n",
            " Train Epoch 34 Summary:  \t Dice Loss: 0.8728\t AVG Dice Coeff: 12.7207\n",
            "\n",
            " Validation Epoch 34 Summary:  \t Dice Loss: 0.8712\t AVG Dice Coeff: 12.8772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX6Cj1VO6vrZ",
        "colab_type": "text"
      },
      "source": [
        "## Expierimental setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXZEDy7v6OZm",
        "colab_type": "text"
      },
      "source": [
        "The baseline approach:\n",
        "\n",
        "1. Majority Voting \n",
        "2. Random shuffling 80% train 20% val split\n",
        "3. 512x512x3 input patches as used in the original paper of 2d-Unet\n",
        "4. Generate sample dataset rougly ~50 patches per input image\n",
        "5. Train with Unet without data augmentation\n",
        "6. Since input 2d grid sizes vary batch size of one will be used?\n",
        "\n",
        "Hyperparameter tuning will *not* be applied is it is considered out of the scope of this assignment.\n",
        "\n",
        "\n",
        "After the baseline expiriment further ideas/practices can be tested:\n",
        "\n",
        "1. Split the dataset based on slice number and *not* randomly!\n",
        "2. Apply Common data augmentation techniques\n",
        "3. Examine input downsampling option\n",
        "4. Use more recent model architectures and compare them to the baseline\n",
        "5. Multiscale feature extraction would be a cool idea since image dimension is high\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeTOmBLhvKzL",
        "colab_type": "text"
      },
      "source": [
        "## Problems Encountered\n",
        "\n",
        "Even though is is shown that 25GB of RAM are available I could not store more than one inputs patche per image in memory.\n",
        "\n",
        "I tried to save only the crop width and height but then then loader was really slow (4 sec to load,crop and preprocess the image  and 2,5 sec with 2 workers- which is still slow).\n",
        "\n",
        "So I wrote the code to store only one patch per train image and changed the training patches every 50 subepochs.(similar to https://arxiv.org/abs/1804.02967)\n",
        "\n",
        "The optimal solution as I see it now would be to store the preprocessed generated image patches in my drive and load at runtime!\n",
        "\n",
        "There was also a problem with annotations that took me some time to figure out. The annotations were different from each expert. I used as a reference the Maps1 folder, because it had the same size(244 labels as the number of images). However, the annotated images were not excacly the same as the train images. Then I used as a reference the Maps5 folder and excluded the extra annotations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th1bVqtaf4sL",
        "colab_type": "text"
      },
      "source": [
        "## Tests\n",
        "\n",
        "#### Some code snippets I tried during writing this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ0ZfbrJf5-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decide majority voting method for relatively small lists\n",
        "a = [1, 2, 3, 1, 2, 1, 1, 1, 3, 2, 2, 1]\n",
        "print(np.bincount(a).argmax())\n",
        "print(max(map(lambda val: (a.count(val), val), set(a)))[1])\n",
        "%timeit np.bincount(a).argmax()\n",
        "%timeit max(map(lambda val: (a.count(val), val), set(a)))[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE9BXC6B6Ng_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model inference to check layers, parameters, memory etc..\n",
        "in_channels = 3\n",
        "model = Unet(in_channels,5).cuda()\n",
        "summary(model,(in_channels,512,512))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pmkpOCH_QrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle train imgs with labels to split the dataset\n",
        "import random\n",
        "\n",
        "a = ['a', 'b', 'c']\n",
        "b = [1, 2, 3]\n",
        "\n",
        "c = list(zip(a, b))\n",
        "random.shuffle(c)\n",
        "a, b = zip(*c)\n",
        "print(a)\n",
        "print(b)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70pFmhB90I3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test modiefied 2d dice loss\n",
        "criterion = DiceLoss2D(2)\n",
        "target = torch.eye(2,2).unsqueeze(0)\n",
        "pred = torch.rand(2,2,2)\n",
        "loss, per_ch = criterion(pred, target)\n",
        "\n",
        "for i in range(10000):\n",
        "  pred = torch.softmax(torch.rand(2,2,2),dim=0)\n",
        "  loss, per_ch = criterion(pred, target)\n",
        "  loss_avg = loss.item()\n",
        "print(loss_avg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxq3cRPKc-Fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data preparation\n",
        "train_imgs = sorted(glob.glob(root_path + 'Train Imgs/Train Imgs/*.jpg'))\n",
        "labels_final = sorted(glob.glob(root_path + 'Labels/*.png'))\n",
        "print(len(train_imgs))\n",
        "print(len(labels_final))\n",
        "for i in range(len(labels_final)):\n",
        "  label_path = labels_final[i]\n",
        "  img_path = train_imgs[i]\n",
        "  shape1 = imageio.imread(img_path).shape\n",
        "  shape2 = imageio.imread(label_path).shape\n",
        "  im1 = img_path.split('_')[4]\n",
        "  h1,w1,d1 = shape1\n",
        "  lab1 = label_path.split('_')[4]\n",
        "  h2,w2 = shape2\n",
        "  print(img_path)\n",
        "  print(label_path)\n",
        "  \n",
        "  if (h1-h2!=0) or (w1-w2!=0):\n",
        "    print(\"ERROR\")\n",
        "    print(\"ERROR\")\n",
        "    print(\"ERROR\")\n",
        "    print(\"ERROR\")\n",
        "    print(\"ERROR\")\n",
        "    print(shape1,shape2)\n",
        "    print(im1,lab1)\n",
        "  else:\n",
        "    print(im1,lab1)\n",
        "    print('------')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}